{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3433b1-40f6-4aaf-88db-b5430b3d22b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import openeo\n",
    "import folium\n",
    "import json\n",
    "import shapely.geometry\n",
    "from openeo.processes import ProcessBuilder\n",
    "import os\n",
    "\n",
    "\n",
    "# connect to openeo\n",
    "conn = openeo.connect(\"https://openeo.vito.be\").authenticate_oidc()\n",
    "\n",
    "# out dir\n",
    "outdir = \"./input/senales\"\n",
    "\n",
    "# aoi\n",
    "aoi = json.load(open('input/senales/senales_wgs84.geojson'))\n",
    "\n",
    "# check the aoi\n",
    "region = aoi['features'][0]['geometry']\n",
    "geom = shapely.geometry.shape(region)\n",
    "centroid = geom.centroid\n",
    "center_latlon = [centroid.y, centroid.x]\n",
    "\n",
    "m = folium.Map(location=center_latlon, zoom_start=9)\n",
    "\n",
    "folium.GeoJson(aoi).add_to(m)\n",
    "m\n",
    "\n",
    "# define time period\n",
    "time_period = ['2024-04-02', '2024-04-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85b4c760-af4b-4d94-b25d-dced7b8244fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2 = conn.load_collection(\n",
    "    'SENTINEL2_L1C',\n",
    "    spatial_extent=region,\n",
    "    temporal_extent=time_period,\n",
    "    bands=[\"B02\", \"B03\", \"B04\", \"B08\", \"B11\"])\n",
    " \n",
    "s2_L2A = conn.load_collection(\n",
    "    'SENTINEL2_L2A',\n",
    "    spatial_extent=region,\n",
    "    temporal_extent=time_period,\n",
    "    bands=['SCL'])\n",
    "\n",
    "worldcover = conn.load_collection(\n",
    "    'ESA_WORLDCOVER_10M_2021_V2',\n",
    "    spatial_extent=region,\n",
    "    bands=['MAP']).resample_spatial(projection=32632)\n",
    "\n",
    "dem = conn.load_collection(\n",
    "    \"COPERNICUS_30\",\n",
    "    spatial_extent=region,\n",
    "    bands=[\"DEM\"]).resample_spatial(projection=32632)\n",
    "\n",
    "water_mask = (worldcover == 80).reduce_dimension(dimension=\"t\", reducer=\"mean\")\n",
    "\n",
    "s2 = s2.merge_cubes(s2_L2A).merge_cubes(water_mask)\n",
    "\n",
    "green = s2.band(\"B03\")\n",
    "swir = s2.band(\"B11\")\n",
    "nir = s2.band(\"B08\")\n",
    "scl = s2.band(\"SCL\")\n",
    "water = s2.band(\"MAP\")\n",
    "\n",
    "# NDSI\n",
    "ndsi = (green - swir) / (green + swir)\n",
    "\n",
    "# cloud mask\n",
    "cloud_mask = ( (scl == 8) | (scl == 9) | (scl == 3) | (scl == 10) ) * 1.0 # times one forces to binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63732486-c72d-4d94-b5b2-306abe6a0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_mask = (~cloud_mask) & (water !=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75e3f9b5-ccd2-4250-b34b-445ba06b3803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_mask.download(os.path.join(outdir,'valid_mask.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c52a5962-a141-4b24-8043-dc22a6fba4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_sure = (ndsi > 0.6) & (nir > 0.45) & valid_mask\n",
    "no_snow_sure = (ndsi < 0) & valid_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888c97c9-0cc7-48b6-8099-0c7222a739e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snow_sure.download(os.path.join(outdir,'snow_sure.nc'))\n",
    "# no_snow_sure.download(os.path.join(outdir,'no_snow_sure.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea229be-02f1-4a1c-b5c4-a1c7887a7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine to a snow_map: 0 = uncertain, 1 = sure no-snow, 2 = sure snow\n",
    "snow_map = snow_sure.multiply(2) + no_snow_sure.multiply(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54cecac3-540d-422e-81f8-f3ec03eebd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snow_map.download(os.path.join(outdir,'snow_map.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f31ac56e-92b8-4ec3-ac6f-0552e04a4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # distance \n",
    "\n",
    "# distance_udf = openeo.UDF.from_file(\"distance_udf.py\")\n",
    "\n",
    "# distance = snow_sure.apply_neighborhood(process=distance_udf,\n",
    "#                                         size=[{\"dimension\": \"x\", \"value\": 2048, \"unit\": \"px\"},\n",
    "#                                               {\"dimension\": \"y\", \"value\": 2048, \"unit\": \"px\"},],\n",
    "#                                         overlap=[{\"dimension\": \"x\", \"value\": 1024, \"unit\": \"px\"},\n",
    "#                                                  {\"dimension\": \"y\", \"value\": 1024, \"unit\": \"px\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ae298c-800d-4926-babc-ada41ffe7c68",
   "metadata": {},
   "source": [
    "The computed distance looks a bit strange. In the upper left and lower right corner, the distance starts to decrease even though there is no snow in that area? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efa7fa0a-6392-457a-8948-99483e37c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance.download('input/distance_normalized.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ebcff3-3dce-4ece-b43c-8d2972757148",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae78382-e504-4f10-9b59-1d4e036bdffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # distance \n",
    "\n",
    "# distance_udf = openeo.UDF.from_file(\"distance_udf.py\")\n",
    "\n",
    "# distance = snow_sure.apply_neighborhood(process=distance_udf,\n",
    "#                                         size=[{\"dimension\": \"x\", \"value\": 512, \"unit\": \"px\"},\n",
    "#                                               {\"dimension\": \"y\", \"value\": 512, \"unit\": \"px\"},],\n",
    "#                                         overlap=[{\"dimension\": \"x\", \"value\": 256, \"unit\": \"px\"},\n",
    "#                                                  {\"dimension\": \"y\", \"value\": 256, \"unit\": \"px\"}]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8b73fc8-a0d5-4441-97a8-17c811460275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance = distance.save_result(format=\"NetCDF\")\n",
    "# job = distance.create_job()\n",
    "# job.start_and_wait()\n",
    "# job.download_results(\"distance/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bd01bb5-3143-4cce-9613-d09700358c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalize distance to range 0f 0-1\n",
    "# distance_min = distance.reduce_dimension(\"t\", reducer=\"min\")\n",
    "# distance_max = distance.reduce_dimension(\"t\", reducer=\"max\")\n",
    "\n",
    "# normalized_distance = (distance - distance_min) / (distance_max - distance_min)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f11450-2f9f-491f-9177-fe5264629a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # normalized_distance.download('input/normalized_distance.nc')\n",
    "# normalized_distance = normalized_distance.save_result(format=\"NetCDF\")\n",
    "# job = distance.create_job()\n",
    "# job.start_and_wait()\n",
    "# job.download_results(\"input/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a543bda-a5c4-4acf-8a1d-7f86ac3182db",
   "metadata": {},
   "source": [
    "Here I decided to switch to single date datacube instead of the month I previously had. Basically all the problems so far where because of the multiple time steps. And now the final straw was that masking the dem with snow_sure would require the dem also to have the same time steps as the snow_sure. I tried to add them but couldn't figure it out. So only one day now instead of multiple days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a53281-dc64-40c5-83b1-b9e11ae5cb22",
   "metadata": {},
   "source": [
    "### mask dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5710289-fdc5-48af-b642-bef980b78972",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_mask = snow_sure.reduce_dimension(\"t\", reducer=\"mean\") # snowmap and dem have different time values in 't', so reduce 't'\n",
    "# snow_mask.download(os.path.join(outdir,'snowmask.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4a6e738-abc9-4f45-b75d-6be5dda05efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem = dem.reduce_dimension(dimension='t', reducer='mean')\n",
    "# dem.download(os.path.join(outdir,'dem.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95572573-fc05-4760-85cf-40bcb59d54de",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_dem = dem.mask(snow_sure==0) # get dem for confident snow areas only\n",
    "# masked_dem.download(os.path.join(outdir,'masked_dem.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "baa7f9b4-99ca-451c-a9b4-4d42f568844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(masked_dem.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b9470f6-8ad2-4915-b333-164657741e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert snow_sure to polygon\n",
    "# snow_polygon = snow_sure.mask(snow_sure == 0).raster_to_vector()\n",
    "# snow_polygon.download(os.path.join(outdir, 'snow_polygon.geojson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f64e1da4-bd9f-4053-96f3-3bdc487ed4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(snow_polygon.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed55f4a-3169-4826-bf95-78055940986d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf351182-88c6-4a9a-a4d7-4b6e2174e276",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### align dem time with snow_sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab480f92-a829-4dba-b989-c440d70dec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_sure = snow_sure.add_dimension(name=\"bands\", label=\"snow_sure\", type=\"bands\")\n",
    "dem = dem.rename_labels(\"bands\", [\"DEM\"]) \n",
    "combined = dem.merge_cubes(snow_sure)\n",
    "\n",
    "# combined = dem.rename_labels(\"bands\", [\"DEM\"]).merge_cubes(\n",
    "#     masked_dem.rename_labels(\"bands\", [\"masked_DEM\"])\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afb027fd-0224-4b4a-9dc2-b6d6ea893466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CollectionMetadata({'spatial': {'bbox': [[-180.0013889, -89.9998611, 179.9986111, 84.0001389]]}, 'temporal': {'interval': [['2010-12-12T00:00:00Z', None]]}} - ['DEM', 'snow_sure'] - ['bands', 't', 'x', 'y'])\n"
     ]
    }
   ],
   "source": [
    "print(combined.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25d7c9af-05af-48ae-a5d5-e39d53b71d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_poly = json.load(open('input/senales/snow_polygon.geojson'))\n",
    "\n",
    "import json\n",
    "from shapely.geometry import shape, mapping\n",
    "from pyproj import Transformer\n",
    "from shapely.ops import transform\n",
    "project = Transformer.from_crs(\"EPSG:32632\", \"EPSG:4326\", always_xy=True).transform\n",
    "\n",
    "# Apply transformation\n",
    "geom = shape(snow_poly[\"features\"][0][\"geometry\"])  # assumes one feature\n",
    "geom_wgs84 = transform(project, geom)\n",
    "\n",
    "# Convert back to GeoJSON format\n",
    "snow_poly_wgs84 = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": [{\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": mapping(geom_wgs84),\n",
    "        \"properties\": {}\n",
    "    }]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7a4fd5a2-6250-4f9b-8223-5898051986ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "altitude_mask_udf = openeo.UDF.from_file(\"altitude_mask_udf.py\")\n",
    "altitude_mask = combined.apply_polygon(geometries=snow_poly_wgs84, process=altitude_mask_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74c31f8d-cf4c-4973-aa59-2737bead68c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one runs but gives boxy look due to chunking\n",
    "# altitude_mask_udf = openeo.UDF.from_file(\"altitude_mask_udf.py\")\n",
    "\n",
    "# altitude_mask = combined.apply_neighborhood(process=altitude_mask_udf,\n",
    "#                                         size=[{\"dimension\": \"x\", \"value\": 256, \"unit\": \"px\"},\n",
    "#                                               {\"dimension\": \"y\", \"value\": 256, \"unit\": \"px\"},],\n",
    "#                                         overlap=[{\"dimension\": \"x\", \"value\": 128, \"unit\": \"px\"},\n",
    "#                                                  {\"dimension\": \"y\", \"value\": 128, \"unit\": \"px\"}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2849fcd-13b8-4828-9ab6-a4f66abcdfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# altitude_mask = altitude_mask.save_result(format=\"NetCDF\")\n",
    "# job = altitude_mask.create_job()\n",
    "# job.start_and_wait()\n",
    "# job.download_results(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e9f762a-2d5b-419b-a21d-56f3ec0f8a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "altitude_mask.download(os.path.join(outdir, 'altitude_mask.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7f686-4ce1-469d-978b-46b9acad166a",
   "metadata": {},
   "source": [
    "i get the chunking problem again with the altitude mask. i tried with apply and get very chunky results. then tried apply_neighborhood and results are less chunky but still chunky and again I would have the problem of increasing the size and overlap in larger areas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1878b2-547d-4def-bae5-ec8f57d418a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### index of distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f33506f9-012b-4e50-b50a-07e1a33a1d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Combine normalized and altitude into index of distance\n",
    "index_of_distance = normalized_distance * altitude_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a083c5d-9129-4b3f-89cc-fe333fac7fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_distance.download('input/distance_index.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a7c21ca-9f8d-4cac-8f03-df92366a38b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataCube' object has no attribute 'round'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m index_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mindex_of_distance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_scale_range\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m254\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mround\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataCube' object has no attribute 'round'"
     ]
    }
   ],
   "source": [
    "index_scaled = index_of_distance.linear_scale_range(0, 1, 0, 254).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe434ce7-e8a2-4369-8534-cd8dbbe3a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adiacency_indexes(curr_acquisition, curr_aux_folder, auxiliary_folder_path, no_data_mask, bands):\n",
    "    sensor = get_sensor(os.path.basename(curr_acquisition))\n",
    "    \n",
    "    path_cloud_mask = glob.glob(os.path.join(curr_aux_folder, '*cloud_mask.nc'))[0]\n",
    "    path_water_mask = glob.glob(os.path.join(auxiliary_folder_path, '*water_mask.nc'))[0]\n",
    "    NDSI_path = glob.glob(os.path.join(curr_aux_folder, '*ndsi.nc'))[0]\n",
    "    dem_path = glob.glob(os.path.join(auxiliary_folder_path, '*dem.nc'))[0]\n",
    "    \n",
    "    valid_mask = np.logical_not(no_data_mask)\n",
    "    \n",
    "    # Load masks and other necessary data\n",
    "    cloud_mask, curr_image_info = open_image(path_cloud_mask)\n",
    "    water_mask = open_image(path_water_mask)[0]\n",
    "    curr_scene_valid = np.logical_not(np.logical_or.reduce((cloud_mask == 2, water_mask == 1, no_data_mask)))\n",
    "    dem = open_image(dem_path)[0]\n",
    "    NDSI = open_image(NDSI_path)[0]\n",
    "    NIR = bands['NIR']\n",
    "    \n",
    "    # Create the snow map\n",
    "    snow_map = np.zeros_like(NDSI, dtype=np.uint8)\n",
    "    no_snow_sure = (NDSI < 0) & curr_scene_valid\n",
    "    snow_sure = (NDSI > 0.6) & (NIR > 0.45) & curr_scene_valid\n",
    "    snow_map[no_snow_sure] = 1\n",
    "    snow_map[snow_sure] = 2\n",
    "\n",
    "    # Calculate distance from snow_sure\n",
    "    distance_from_snow = np.full_like(snow_map, np.nan, dtype=np.float32)\n",
    "    snow_sure_pixels = (snow_map == 2)\n",
    "    distance_from_snow[curr_scene_valid] = distance_transform_edt(~snow_sure_pixels)[curr_scene_valid]\n",
    "    distance_from_snow = np.nan_to_num(distance_from_snow, nan=np.nanmax(distance_from_snow))\n",
    "    distance_from_snow_normalized = (distance_from_snow - np.nanmin(distance_from_snow)) / (\n",
    "        np.nanmax(distance_from_snow) - np.nanmin(distance_from_snow)\n",
    "    )\n",
    "    \n",
    "    # Set altitude threshold\n",
    "    valid_dem = dem[np.logical_and(curr_scene_valid, snow_map == 2)]\n",
    "    \n",
    "    if valid_dem.size > 0:\n",
    "        altitude_min_threshold = np.percentile(valid_dem, 1) - 200\n",
    "    else:\n",
    "        altitude_min_threshold = np.nan  # Oppure scegli un valore predefinito sensato\n",
    "    \n",
    "    altitude_mask = (dem >= altitude_min_threshold) if not np.isnan(altitude_min_threshold) else np.zeros_like(dem, dtype=bool)\n",
    "\n",
    "\n",
    "    \n",
    "    altitude_mask = (dem >= altitude_min_threshold)\n",
    "    \n",
    "    # Combine distance and altitude into index_of_distance\n",
    "    index_of_distance = np.zeros_like(snow_map, dtype=np.float32)\n",
    "    index_of_distance[curr_scene_valid] = (\n",
    "        distance_from_snow_normalized[curr_scene_valid] * altitude_mask[curr_scene_valid]\n",
    "    )\n",
    "    \n",
    "   \n",
    "    # Convert to uint8 for saving\n",
    "    index_of_distance_uint8 = (index_of_distance * 254).astype(np.uint8)  # Scale if needed\n",
    "    \n",
    "    # Set no-data value for areas outside altitude_mask\n",
    "    no_data_value = 255  # Choose the no-data value, e.g., 0 or 255\n",
    "    index_of_distance_uint8[np.logical_or(~altitude_mask, ~curr_scene_valid)] = no_data_value\n",
    "\n",
    "    \n",
    "    # Save the result as a GeoTIFF\n",
    "    output_path = os.path.join(curr_aux_folder, \"index_of_distance.tif\")\n",
    "    transform = from_origin(curr_image_info['geotransform'][0], curr_image_info['geotransform'][3], \n",
    "                            curr_image_info['geotransform'][1], -curr_image_info['geotransform'][5])\n",
    "    with rasterio.open(\n",
    "        output_path,\n",
    "        \"w\",\n",
    "        driver=\"GTiff\",\n",
    "        height=index_of_distance_uint8.shape[0],\n",
    "        width=index_of_distance_uint8.shape[1],\n",
    "        count=1,\n",
    "        dtype=rasterio.uint8,\n",
    "        crs=curr_image_info['projection'],\n",
    "        transform=transform,\n",
    "        nodata=no_data_value,\n",
    "    ) as dst:\n",
    "        dst.write(index_of_distance_uint8, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec5f62-d32d-434a-80c3-3e3f373d84e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
